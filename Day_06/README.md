# Classification Loss Functions

## Topics covered in today's module
* Kullback Leibler Divergence Loss
* Binary Cross-Entropy
* Categorical Cross-Entropy
* Sparse Categorical Cross-Entropy

## Main takeaways from doing today's assignment
1. Kullback Leibler Divergence Loss
    *A Kullback-Leibler divergence has Zero loss and it indicates that both probability distributions are the same.
2. Binary Cross-Entropy
    * Binary cross entropy is a loss function that is used in binary classification tasks.
3. Categorical Cross-Entropy
    * Most commonly used for classification problems.  
4. Sparse Categorical Cross-Entropy
    *Same thing as CCE however the formula format is slightly different.

## Challenging, interesting, or exciting aspects of today's assignment
<To be filled>

## Additional resources used 
<To be filled>
